{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfred.data.zoo.alfred import AlfredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    name = 'default'\n",
    "    # model to use\n",
    "    model = 'transformer'\n",
    "    # which device to use\n",
    "    device = 'cuda'\n",
    "    # number of data loading workers or evaluation processes (0 for main thread)\n",
    "    num_workers = 0\n",
    "    # we can fine-tune a pre-trained model\n",
    "    pretrained_path = None\n",
    "    # run the code on a small chunk of data\n",
    "    fast_epoch = False\n",
    "\n",
    "    # DATA SETTINGS\n",
    "    data = {\n",
    "        # dataset name(s) for training and validation\n",
    "        'train': None,\n",
    "        # additional dataset name(s) can be specified for validation only\n",
    "        'valid': '',\n",
    "        # specify the length of each dataset\n",
    "        'length': 800,\n",
    "        # what to use as annotations: {'lang', 'lang_frames', 'frames'}\n",
    "        'ann_type': 'lang',\n",
    "    }\n",
    "    \n",
    "    seed = 14\n",
    "    # load a checkpoint from a previous epoch (if available)\n",
    "    resume = True\n",
    "    # whether to print execution time for different parts of the code\n",
    "    profile = False\n",
    "\n",
    "    # HYPER PARAMETERS\n",
    "    # batch size\n",
    "    batch = 8\n",
    "    # number of epochs\n",
    "    epochs = 200\n",
    "    # optimizer type, must be in ('adam', 'adamw')\n",
    "    optimizer = 'adamw'\n",
    "    # L2 regularization weight\n",
    "    weight_decay = 0.33\n",
    "    # learning rate settings\n",
    "    lr = {\n",
    "        # learning rate initial value\n",
    "        'init': 1e-3,\n",
    "        # lr scheduler type: {'linear', 'cosine', 'triangular', 'triangular2'}\n",
    "        'profile': 'linear',\n",
    "        # (LINEAR PROFILE) num epoch to adjust learning rate\n",
    "        'decay_epoch': 10,\n",
    "        # (LINEAR PROFILE) scaling multiplier at each milestone\n",
    "        'decay_scale': 0.8,\n",
    "        # (COSINE & TRIANGULAR PROFILE) learning rate final value\n",
    "        'final': 1e-5,\n",
    "        # (TRIANGULAR PROFILE) period of the cycle to increase the learning rate\n",
    "        'cycle_epoch_up': 0,\n",
    "        # (TRIANGULAR PROFILE) period of the cycle to decrease the learning rate\n",
    "        'cycle_epoch_down': 0,\n",
    "        # warm up period length in epochs\n",
    "        'warmup_epoch': 0,\n",
    "        # initial learning rate will be divided by this value\n",
    "        'warmup_scale': 1,\n",
    "    }\n",
    "    # weight of action loss\n",
    "    action_loss_wt = 1.\n",
    "    # weight of object loss\n",
    "    object_loss_wt = 1.\n",
    "    # weight of subgoal completion predictor\n",
    "    subgoal_aux_loss_wt = 0.1\n",
    "    # weight of progress monitor\n",
    "    progress_aux_loss_wt = 0.1\n",
    "    # maximizing entropy loss (by default it is off)\n",
    "    entropy_wt = 0.0\n",
    "\n",
    "    # TRANSFORMER settings\n",
    "    # size of transformer embeddings\n",
    "    demb = 768\n",
    "    # number of heads in multi-head attention\n",
    "    encoder_heads = 12\n",
    "    # number of layers in transformer encoder\n",
    "    encoder_layers = 2\n",
    "    # how many previous actions to use as input\n",
    "    num_input_actions = 1\n",
    "    # which encoder to use for language encoder (by default no encoder)\n",
    "    encoder_lang = {\n",
    "        'shared': True,\n",
    "        'layers': 2,\n",
    "        'pos_enc': True,\n",
    "        'instr_enc': False,\n",
    "    }\n",
    "    # which decoder to use for the speaker model\n",
    "    decoder_lang = {\n",
    "        'layers': 2,\n",
    "        'heads': 12,\n",
    "        'demb': 768,\n",
    "        'dropout': 0.1,\n",
    "        'pos_enc': True,\n",
    "    }\n",
    "    # do not propagate gradients to the look-up table and the language encoder\n",
    "    detach_lang_emb = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset was recorded using model at /home/georgiynefedov/RP//logs/pretrained/fasterrcnn_model.pth\n",
      "train dataset size = 1068826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgiynefedov/anaconda3/envs/rp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = ARGS()\n",
    "dataset = AlfredDataset('lmdb_human', 'train', args, 'lang')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'LookDown_15': 63619,\n",
       "             'MoveAhead_25': 624869,\n",
       "             'RotateLeft_90': 89704,\n",
       "             'LookUp_15': 39997,\n",
       "             'PickupObject': 39427,\n",
       "             'RotateRight_90': 94301,\n",
       "             'ToggleObjectOn': 8013,\n",
       "             '<<stop>>': 20958,\n",
       "             'OpenObject': 20922,\n",
       "             'CloseObject': 21148,\n",
       "             'PutObject': 37064,\n",
       "             'SliceObject': 3042,\n",
       "             'ToggleObjectOff': 5762})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5718574639651676e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.1147774904129135e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 0.00012479720454261825,\n",
       " 4.7714476572192006e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.1147774904129135e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 0.00012479720454261825,\n",
       " 4.7714476572192006e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.1147774904129135e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 0.00012479720454261825,\n",
       " 4.7714476572192006e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 0.00012479720454261825,\n",
       " 4.7714476572192006e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 0.00012479720454261825,\n",
       " 4.7714476572192006e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 0.00012479720454261825,\n",
       " 4.7714476572192006e-05,\n",
       " 1.5718574639651676e-05,\n",
       " 1.0604341417376274e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.0604341417376274e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.5718574639651676e-05,\n",
       " 2.5363329697922742e-05,\n",
       " 2.5001875140635547e-05,\n",
       " 1.0604341417376274e-05,\n",
       " 1.0604341417376274e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.1147774904129135e-05,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06,\n",
       " 1.6003354303061922e-06]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sampler_weights[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
